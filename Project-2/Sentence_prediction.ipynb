{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ab8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1\n",
    "\n",
    "from urllib.request import urlopen\n",
    "txt = urlopen(\"https://raw.githubusercontent.com/crash-course-ai/lab2-nlp/master/vlogbrothers.txt\").read().decode('ascii').split(\"\\n\")\n",
    "print(\"Our dataset contains {} vlogbrothers scripts\".format(len(txt)))\n",
    "# ADVANCED_CHANGEME -- You can change this to load any text file\n",
    "# You want it to be one line of plain text for every script\n",
    "# Extra annotations like [John:] or *starts coughing* make learning more defficult\n",
    "everything = set([w for s in txt for w in s.split()])\n",
    "print(\"and {} lexical types\".format(len(everything)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bdf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.2\n",
    "\n",
    "# 1. Import the tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\",\"tagger\", \"ner\", \"textcat\"])\n",
    "\n",
    "# 2. Tokenize\n",
    "txt = [nlp(s) for s in txt]\n",
    "\n",
    "# 3. Mark the beggining and end of each script\n",
    "txt = [ [\"<s>\"] + [str(w) for w in s] + [\"</s>\"] for s in txt]\n",
    "\n",
    "# 4. Separate the data into training and validation\n",
    "train = txt[:-5]\n",
    "valid = txt[-5:]\n",
    "\n",
    "# 5. Flatten the lists into one long string and remove extra whitespace\n",
    "train = [w for s in train for w in s if not w.isspace()]\n",
    "valid = [w for s in valid for w in s if not w.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3\n",
    "\n",
    "\"\"\"\n",
    "    How big is our dataset?\n",
    "\"\"\"\n",
    "print(\"Our training set contains {} lexical types\".format(len(set(train))))\n",
    "print(\"Our training set contains {} lexical tokens\".format(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4105546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4\n",
    "\n",
    "# 1. Count the frequency of every word\n",
    "from collections import Counter, defaultdict\n",
    "counts = Counter(train)\n",
    "\n",
    "frequencies = [0]*8\n",
    "for w in counts:\n",
    "    if counts[w] >= 128:\n",
    "        frequencies[0] += 1\n",
    "    elif counts[w] >= 64:\n",
    "        frequencies[1] += 1\n",
    "    elif counts[w] >= 32:\n",
    "        frequencies[2] += 1\n",
    "    elif counts[w] >= 16:\n",
    "        frequencies[3] += 1\n",
    "    elif counts[w] >= 8:\n",
    "        frequencies[4] += 1\n",
    "    elif counts[w] >= 4:\n",
    "        frequencies[5] += 1\n",
    "    elif counts[w] >= 2:\n",
    "        frequencies[6] += 1\n",
    "    else:\n",
    "        frequencies[7] += 1\n",
    "\n",
    "# 2. Plot their distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "f,a = plt.subplots(1,1, figsize=(10,5))\n",
    "a.set(xlabel='Lexical types occuring more than n times', ylabel='Number of lexical types')\n",
    "\n",
    "labels = [\"128\", \"64\", \"32\", \"16\", \"8\", \"4\", \"2\", \"1\"]\n",
    "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67295ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.5\n",
    "\n",
    "from textwrap import wrap\n",
    "rare = [w for w in counts if counts[w] == 1]\n",
    "for line in wrap(\"   \".join([\"{:15s}\".format(w) for w in rare[:100]]), width=70):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00748ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.6\n",
    "\n",
    "# This is a little function to help us clean up the data\n",
    "# CHANGEME == Introduce or remove rules\n",
    "import re\n",
    "def simplify(w):\n",
    "    # Remove punctuation\n",
    "    w = w.replace(\"-\", \"\").replace(\"~\", \"\")\n",
    "\n",
    "    # Replace numbers with # sign\n",
    "    w = re.sub('\\d', '#', w)\n",
    "\n",
    "    # Change some endings\n",
    "    if len(w) > 3 and w[-2:] in set([\"ed\", \"er\", \"ly\"]):\n",
    "        return [w[:-2], w[-2:]]\n",
    "    elif len(w) > 4 and w[-3:] in set([\"ing\", \"'re\"]):\n",
    "        return [w[:-3], w[-3:]]\n",
    "    return [w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.7\n",
    "\n",
    "# 1. Go through and clean all of our data\n",
    "train_clean = []\n",
    "for w in train:\n",
    "    for piece in simplify(w):\n",
    "        train_clean.append(piece)\n",
    "valid_clean = []\n",
    "for w in valid:\n",
    "    for piece in simplify(w):\n",
    "        valid_clean.append(piece)\n",
    "\n",
    "\"\"\"\n",
    "    How big is our dataset?\n",
    "\"\"\"\n",
    "print(\"{} lexical types\". format(len(set(train_clean))))\n",
    "print(\"{} lexical tokens\".format(len(train_clean)))\n",
    "\n",
    "\"\"\"\n",
    "    What's our distribution looks like?\n",
    "\"\"\"\n",
    "counts = Counter(train_clean)\n",
    "\n",
    "frequencies = [0]*8\n",
    "for w in counts:\n",
    "    if counts[w] >= 128:\n",
    "        frequencies[0] += 1\n",
    "    elif counts[w] >= 64:\n",
    "        frequencies[1] += 1\n",
    "    elif counts[w] >= 32:\n",
    "        frequencies[2] += 1\n",
    "    elif counts[w] >= 16:\n",
    "        frequencies[3] += 1\n",
    "    elif counts[w] >= 8:\n",
    "        frequencies[4] += 1\n",
    "    elif counts[w] >= 4:\n",
    "        frequencies[5] += 1\n",
    "    elif counts[w] >= 2:\n",
    "        frequencies[6] += 1\n",
    "    else:\n",
    "        frequencies[7] += 1\n",
    "\n",
    "# 2. Plot their distributions\n",
    "f,a = plt.subplots(1,1, figsize=(10,5))\n",
    "a.set(xlabel='Lexical types occuring more than n times', ylabel='Number of lexical types')\n",
    "\n",
    "labels = [\"128\", \"64\", \"32\", \"16\", \"8\", \"4\", \"2\", \"1\"]\n",
    "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.8\n",
    "\n",
    "counts_clean = Counter(train_clean)\n",
    "train_unk = [w if counts_clean[w] > 1 else \"unk\" for w in train_clean]\n",
    "valid_unk = [w if w in count_clean and count_clean[w] > 1 else \"unk\" for w in valid_clean]\n",
    "\n",
    "# Let's plot these one last time\n",
    "frequencies = [0]*8\n",
    "for w in counts_clean:\n",
    "    if counts_clean[w] >= 128:\n",
    "        frequencies[0] += 1\n",
    "    elif counts_clean[w] >= 64:\n",
    "        frequencies[1] += 1\n",
    "    elif counts_clean[w] >= 32:\n",
    "        frequencies[2] += 1\n",
    "    elif counts_clean[w] >= 16:\n",
    "        frequencies[3] += 1\n",
    "    elif counts_clean[w] >= 8:\n",
    "        frequencies[4] += 1\n",
    "    elif counts_clean[w] >= 4:\n",
    "        frequencies[5] += 1\n",
    "    elif counts_clean[w] >= 2:\n",
    "        frequencies[6] += 1\n",
    "    else:\n",
    "        frequencies[7] += 1\n",
    "\n",
    "# 2. Plot their distributions\n",
    "f,a = plt.subplots(1,1, figsize=(10,5))\n",
    "a.set(xlabel='Lexical types occuring more than n times', ylabel='Number of lexical types')\n",
    "\n",
    "labels = [\"128\", \"64\", \"32\", \"16\", \"8\", \"4\", \"2\", \"1\"]\n",
    "_ = sns.barplot(labels, frequencies, ax=a, order=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.9\n",
    "\n",
    "rare = [w for w in counts_clean if counts_clean[w] == 1]\n",
    "rare.sort()\n",
    "for line in wrap(\"   \".join([\"{:15s}\".format(w) for w in rare[-100:]]), width=70):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb76a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1\n",
    "\"\"\"\n",
    "    Prepare our datasets by converting words to numbers\n",
    "\"\"\"\n",
    "# Create a mapping from words <-> numbers\n",
    "vocabulary = set(train_unk)\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "for num, word in enumerate(vocabulary):\n",
    "    word_to_num[word] = num\n",
    "    num_to_word[num] = word\n",
    "\n",
    "# Convert our datasets to numbers\n",
    "import torch\n",
    "train = torch.LongTensor(len(train_unk))\n",
    "for i in range(len(train_unk)):\n",
    "    train[i] = word_to_num[train_unk[i]]\n",
    "\n",
    "valid = torch.LongTensor(len(valid_unk))\n",
    "for i in range(len(valid_unk)):\n",
    "    valid[i] = word_to_num[valid_unk[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d546775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2\n",
    "\n",
    "# Parameters\n",
    "batch_size = 20\n",
    "seq_len = 35        # CHANGEME\n",
    "\n",
    "# Tell Torch to use a GPU for computation\n",
    "device = torch.device(\"cuda\")\n",
    "# Setting the random seed decreases variability\n",
    "# Remove next three lines if runnuing on your laptop\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# citation: https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i, seq_len):\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "train = batchify(train, batch_size)\n",
    "valid = batchify(valid, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.3\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            Define all the parameters of the model\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        # How tightly we compress our language representations?\n",
    "        self.embed_size = 300       # How big is our word vector?   # ADVANCED_CHANGEME\n",
    "        self.hidden_size = 600      # How big is our hidden space?  # ADVANCED_CHANGEME\n",
    "\n",
    "        \"\"\" Converting words to Vectors \"\"\"\n",
    "        # A lookup table for translating words into a vector\n",
    "        self.embedding = nn.Embedding(len(vocabulary), self.embed_size)\n",
    "        # Initialize our word vectors with a random uniform distribution\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        \"\"\" An RNN (LSTM) with dropout \"\"\"\n",
    "        self.rnn = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size)\n",
    "        self.shrink = nn.Linear(self.hidden_size, self.embed_size)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        \"\"\" Predicting words from our model \"\"\"\n",
    "        # We convert our vector to a set of scores over words\n",
    "        self.decode = nn.Linear(self.hidden_size, self.embedding.weight.size(0))\n",
    "        # We use the same matrix for this ``decoding'' that we used for ``encoding''\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        self.decode.weight = self.embedding.weight\n",
    "        self.decode.bias.data.zero_()\n",
    "\n",
    "def forward(self, input, hidden=None):\n",
    "    \"\"\"\n",
    "        Run the model\n",
    "    \"\"\"\n",
    "    # 1. Map words to vectors\n",
    "    embedded = self.embedding(input)\n",
    "    # 2. Process with an RNN\n",
    "    if hidden is not None:\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "    else:\n",
    "        output, hidden = self.rnn(embedded)\n",
    "    # 3. Apply dropout\n",
    "    output = F.relu(self.shrink(self.drop(output)))\n",
    "    # 4. Score the likelihood of every possible next word\n",
    "    decoded = self.decode(output)\n",
    "    return hidden, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b38688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def training(model, data, targets, lr, hidden):\n",
    "    # Reset the model\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Eun the model to see its predictions and hidden states\n",
    "    hidden, prediction_vector = model(data, hidden)\n",
    "    prediction_vector = prediction_vector.view(seq_len * batch_size, -1)\n",
    "\n",
    "    # Compare the model's predictions at each timestep to the original data\n",
    "    loss = F.cross_entropy(prediction_vector, targets)\n",
    "\n",
    "    # Compute gradients and perform back-propagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "    # Return the current model loss on this data item\n",
    "    return loss.item(), repackage_hidden(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2\n",
    "\n",
    "def evaluation(model):\n",
    "    \"\"\"\n",
    "        This function performs almost all the same logic as the training function but it does not perform backpropagation, because we don't want to learn from this data, just check our performance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    valid_loss = 0\n",
    "    for i in range(0, valid.size(0) - seq_len, seq_len):\n",
    "        data, targets = get_batch(valid, i, seq_len)\n",
    "        hidden, prediction_vector = model(data, hidden)\n",
    "        prediction_vector = prediction_vector.view(seq_len * batch_size, -1)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "\n",
    "        prediction_vector = prediction_vector.view(-1, len(vocabulary))\n",
    "        loss = F.cross_entropy(prediction_vector, targets)\n",
    "        valid_loss += loss.item()\n",
    "    return valid_loss / (valid.size(0)/seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3\n",
    "\n",
    "# Create an instance of the model\n",
    "import numpy as np\n",
    "import time\n",
    "model = EncoderDecoder().float().to(device)\n",
    "prev_valid_loss = 1e100\n",
    "# This scales the size of each step of backpropagation\n",
    "learning_rate = 20\n",
    "# This value should match the batch_size used earlier for splitting up the data\n",
    "batch_size = 20\n",
    "\n",
    "num_epochs = 10\n",
    "timing = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Set the model to training mode and iterate through the training dataset\n",
    "    model.train()\n",
    "    hidden = None\n",
    "    train_loss = 0\n",
    "    start_time = time.time()\n",
    "    for i in range(0, train.size(0) - 1, seq_len):\n",
    "        # Get the next training batch\n",
    "        data, targets = get_batch(train, i, seq_len)\n",
    "\n",
    "        # Run the model and perform backpropagation\n",
    "        loss, hidden = training(model, data, targets, learning_rate, hidden)\n",
    "        train_loss += loss\n",
    "\n",
    "    # Evaluate how well the model predicts unseen validation data\n",
    "    valid_loss = evaluation(model)\n",
    "\n",
    "    # Check if the model's ability to generalize has gotten worse.\n",
    "    # If so, slow the learning rate (shrink the step size)\n",
    "    if valid_loss > prev_valid_loss:\n",
    "        learning_rate /= 4.0\n",
    "\n",
    "    # Print the training and validation performance\n",
    "    train_loss /= (train.size(0)/seq_len)\n",
    "    finish_time = time.time()\n",
    "    print(\"Epoch {:2} took {:3.2f}s with train preplixity: {:7.2f} and validation: {:7.2f}\".format(epoch, finish_time - start_time, np.exp(train_loss), np.exp(valid_loss)))\n",
    "    prev_valid_loss = valid_loss\n",
    "\n",
    "total_time = (time.time() - timing)/60\n",
    "print(\"Completed {} epochs in {:5.3f} minutes\".format(num_epochs, total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1\n",
    "\n",
    "# What do we want the model to start the sentence with?\n",
    "prefix = \"<s> Good\" # CHANGEME\n",
    "\n",
    "# How many words do we want the model to produce?\n",
    "words_to_generate = 50 # CHANGEME\n",
    "\n",
    "# we are only going to be lookig at one example at a time\n",
    "batch_size = 1\n",
    "\n",
    "# Set the model to be in evaluation mode (no backprop!)\n",
    "model.eval()\n",
    "\n",
    "# Let's get lots of possible sentences\n",
    "argmax_sent = None\n",
    "argmax_prob = 0\n",
    "collection = []\n",
    "for iten in range(100):\n",
    "    # Convert our sentence start into numbers\n",
    "    test = [word_to_num[w] if word in word_to_num else word_to_num[\"unk\"] for word in prefix.split()]\n",
    "    probabilities = []\n",
    "\n",
    "    # Run the model on the same initial and it's own generations until we reach `words_to_generate`\n",
    "    for w in range(words_to_generate):\n",
    "        # Run the model\n",
    "        input = torch.from_numpy(np.array(test)).to(device)\n",
    "        _, output = model(input.view(-1,1))\n",
    "\n",
    "        # Get the prediction for the last (next) word\n",
    "        last_pred = output[-1,:,:].sequeeze()\n",
    "\n",
    "        # We're going to block generation ok unk\n",
    "        last_pred[word_to_num[\"unk\"]] = -100\n",
    "\n",
    "        # Do we want to sample from this distribution?\n",
    "        if item > 0:\n",
    "            # A tenperature makes the distribution peakier (if < 1) or flatter if > 1\n",
    "            last_pred /= 0.70   # ADVANCED_CHANGEME\n",
    "\n",
    "            # Turn this into a distribution\n",
    "            dist = torch.distributions.categorical.Categorical(logits=last_pred)\n",
    "\n",
    "            # Sample\n",
    "            predicted_idx = dist.sample().item()\n",
    "\n",
    "        else:\n",
    "            # If we aren't sampling, just take the most probable word\n",
    "            _, predicted_idx = last_pred.max(0)\n",
    "            predicted_idx = predicted_idx.item()\n",
    "\n",
    "        # Save the predicted word's probability\n",
    "        value = F.log_softmax(last_pred, -1)[predicted_idx].item()\n",
    "\n",
    "        # Add this predicted word (index) to the list\n",
    "        test\n",
    "        # Save the probability for sorting later\n",
    "        probabilities.append(value)\n",
    "\n",
    "    if item > 0:\n",
    "        # Add our senetnce and its socre to a list\n",
    "        generation = (np.exp(np.sum(probabilities)), \" \".join([num_to_word[w] for w in test]))\n",
    "        if generation not in collection:\n",
    "            collection.append(generation)\n",
    "    else:\n",
    "        argmax_sent = \" \".join([num_to_word[w] for w in test])\n",
    "        argmax_prob = np.exp(np.sum(probabilities))\n",
    "\n",
    "# Get the best model predictions\n",
    "collection.sort()\n",
    "collection.reverse()\n",
    "print(\"Argmax Generation:\")\n",
    "print(\"{:.2E}: {}\\n\".format(argmax_prob, \"\\n\\t\\t\".join(wrap(argmax_sent))))\n",
    "print(\"\\nSample Generations:\")\n",
    "for probability, sent in collection[:10]:\n",
    "    print(\"{:.2E}: {}\\n\".format(probability, \"\\n\\t\\t\".join(wrap(sent))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
